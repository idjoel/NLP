{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jfenata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary had a little lamb.', 'Her fleece was white as snow']\n"
     ]
    }
   ],
   "source": [
    "text=\"Mary had a little lamb. Her fleece was white as snow\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sents=sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mary', 'had', 'a', 'little', 'lamb', '.'], ['Her', 'fleece', 'was', 'white', 'as', 'snow']]\n"
     ]
    }
   ],
   "source": [
    "words=[word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all the stop words\n",
    "\n",
    "NLTK comes with some built in linguistic resources and among those linguistic resources is a collection of stopwords in different languages. You can import the set of stopwords by using the statement from NLTK.\n",
    "\n",
    "Along with the stopwords that are provided by NLTK, we're also importing puctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jfenata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "customStopWords = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given any list of tokens or words that you have got by tokenizing a piece of text, you can take that and just filter it for only those words which are not in the list of custom stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'little', 'lamb', 'Her', 'fleece', 'white', 'snow']\n"
     ]
    }
   ],
   "source": [
    "wordsWithoutStopwords = [word for word in word_tokenize(text) if word not in customStopWords]\n",
    "\n",
    "print(wordsWithoutStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify bigrams\n",
    "\n",
    "N-grams are groups of words that occur commonly together from any piece of text you would want to identify important n-gram which occur in that text.\n",
    "\n",
    "It's how to construct bigrams from a list of words and also see what the frequency of occurence of those bigrams are within that list of words.\n",
    "\n",
    "> Collocations - any words that are collocated or that occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(('Her', 'fleece'), 1),\n (('Mary', 'little'), 1),\n (('fleece', 'white'), 1),\n (('lamb', 'Her'), 1),\n (('little', 'lamb'), 1),\n (('white', 'snow'), 1)]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder =  BigramCollocationFinder.from_words(wordsWithoutStopwords)\n",
    "\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the bigrams which are present within this list of words and their frequencies. So each bigram occurs along with the number of times that it has occured within that list of words.\n",
    "\n",
    "> So each bigram occurs along with the number of times that it has occured within that list of words. If you had a piece of text in which particular bigrams were more important than others then this particular piece of code would sort all of the bigrams in the order of their frequency and you would see the most important bigrams on top.\n",
    "\n",
    "<b>\n",
    "\n",
    "> The collocations module also has a trigram collocation finder, which you can use in a very similar way to find trigrams or group of words in three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Part Of Speech Tagging\n",
    "\n",
    "Different morphological forms of the same word (close, closing & closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'clos', 'on', 'clos', 'night', 'when', 'she', 'was', 'in', 'the', 'mood', 'to', 'clos', '.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Mary closed on closing night when she was in the mood to close.\"\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "st = LancasterStemmer()\n",
    "stemmedWords = [st.stem(word) for word in word_tokenize(text2)]\n",
    "\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinarily if you tokenized this sentence and looked at all the words within them, each of the different morphological forms of the word close would be treated as different tokens. So if you try to perform a count of how many times each word occurs in this sentence, then you would get the counts closed as one, closing as one, and close as one.\n",
    "\n",
    "Now instead if you wanted to treat them all as the same word and get a count of close as three, then you would basically have to reduce all these three words to their root form. \n",
    "This is basically called stemming and the NLTK.stem module has several different algorithms that allow you to perform stemming.\n",
    "\n",
    "We are going to use one particular stemmer called the Lancaster stemmer algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to see whether one particular word is a noun, or a verb, or an adverb, and so on.\n",
    "\n",
    "Once again, NLTK has a built in function for this called part-of-speech tag (pos_tag). All you need to do is pass it a list of words and it will assign the relevant part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jfenata/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('Mary', 'NNP'),\n ('closed', 'VBD'),\n ('on', 'IN'),\n ('closing', 'NN'),\n ('night', 'NN'),\n ('when', 'WRB'),\n ('she', 'PRP'),\n ('was', 'VBD'),\n ('in', 'IN'),\n ('the', 'DT'),\n ('mood', 'NN'),\n ('to', 'TO'),\n ('close', 'VB'),\n ('.', '.')]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here you have all the acronyms thich represent the different part of speech for each the words.\n",
    "\n",
    "> Noun(NNP)\n",
    "Verb(VBD)\n",
    "Pronoun(PRP)\n",
    "\n",
    "<B>\n",
    "\n",
    "> For a complete list of each acronym and word it means, you can look up the NLTK documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8928a8525e82c28dfca912a36a2a2d5e65abfe8c99882cc97d8dc4c1ed24e146"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}